{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed  # for parallel processes\n",
    "import tarfile\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "import re # for preprocessing the corpus\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open(\"../../NLP_data/parallel/en-fr_small.tar.gz\", \"r:gz\")\n",
    "files = tar.getmembers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset = lambda file: tar.extractfile(file).read().decode(\"utf-8\")\n",
    "\n",
    "tokenize = lambda text: text.split(\"\\n\")\n",
    "\n",
    "w2id = lambda vocabs, len_special_tokens: dict(\n",
    "    zip(vocabs, range(len_special_tokens,len(vocabs)+len_special_tokens)))\n",
    "\n",
    "id2w = lambda w2ids: dict([(v,k) for k,v in w2ids.items()])\n",
    "\n",
    "append_EOS_token = lambda sentences: [s + [\"<EOS>\"] for s in sentences]\n",
    "\n",
    "sentence_ids_to_words = lambda ids_list, id2w_dict: \" \".join([id2w_dict[i] for i in ids_list])\n",
    "\n",
    "sentence_to_ids = lambda sentence, w2id_dict:[\n",
    "    w2id_dict[w] if w in w2id_dict else w2id_dict[\"<UNK>\"] for w in sentence]\n",
    "\n",
    "lists_of_ids = lambda sentences, w2id_dict:[\n",
    "    sentence_to_ids(s, w2id_dict) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw:str, translation_table = str.maketrans(\"éàèùâêîôûçşöüı\", \"eaeuaeioucsoui\")):\n",
    "    raw = raw.lower().translate(translation_table)\n",
    "    raw = re.sub(\"\\d+\",\"#\", raw)\n",
    "    raw = re.sub(\"'+\",\"_\", raw)\n",
    "    return re.sub(\"[^A-Za-z_#]\",\" \", raw).split()\n",
    "\n",
    "# converting sentences to wordlists, utilizing all the cpu cores\n",
    "def tokenize_sentences(func, raw_sentences):\n",
    "    return Parallel(n_jobs=-1)(\n",
    "        delayed(func)(\n",
    "            raw_sentence) for raw_sentence in raw_sentences)\n",
    "\n",
    "# Grouping words like \"new\" \"york\" into one word (i.e new_york)\n",
    "def bigram_sentences(tokenized_sentences):\n",
    "    phrases = Phrases(tokenized_sentences)\n",
    "    bigram = Phraser(phrases)\n",
    "    return list(bigram[tokenized_sentences])\n",
    "\n",
    "def get_vocabs(sentences, max_len=10000):\n",
    "    vocabs = {}  # word:freq\n",
    "    for s in sentences:\n",
    "        for w in s:\n",
    "            vocabs.setdefault(w, 0)\n",
    "            vocabs[w] += 1\n",
    "    return [i[0] for i in sorted(vocabs.items(), key=lambda i:i[1], reverse=True)][:max_len], len(vocabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word <==> id Lookup table\n",
    "<img src=\"http://nbviewer.jupyter.org/github/deep-diver/EN-FR-MLT-tensorflow/blob/master/conversion.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {\"<PAD>\":0, \"<EOS>\":1, \"<UNK>\":2, \"<GO>\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(path:str, tokenizer, special_tokens={\"<PAD>\": 0, \"<EOS>\": 1, \"<UNK>\": 2, \"<GO>\": 3}, is_target=False,\n",
    "                 max_num_sentences=None, bigram=False, max_vocab_size=10000, opsrc_mode=\"r\",\n",
    "                 is_tar=False\n",
    "                 ):\n",
    "    # Read the text\n",
    "    if is_tar:\n",
    "        text_raw = load_dataset(path)\n",
    "    else:\n",
    "        with codecs.open(path, opsrc_mode, \"utf-8\") as file:\n",
    "            text_raw = file.read().lower()\n",
    "\n",
    "    # Extract sentences\n",
    "    raw_sentences = tokenizer(text_raw)\n",
    "\n",
    "    orig_len = len(raw_sentences)\n",
    "\n",
    "    raw_sentences = raw_sentences[:max_num_sentences]\n",
    "    print(\"Sentences:\\t{:,}/{:,}\".format(len(raw_sentences), orig_len))\n",
    "\n",
    "    # Tokenize sentence to list of words\n",
    "    tokenized_sentences = tokenize_sentences(sentence_to_wordlist, raw_sentences)\n",
    "\n",
    "    # Group common bigram of words (e.g new + york => new_york)\n",
    "    if bigram:\n",
    "        tokenized_sentences = bigram_sentences(tokenized_sentences)\n",
    "\n",
    "    # Get the unique vocabs in the bigrammed corpus\n",
    "    vocabs, vocab_length = get_vocabs(tokenized_sentences, max_vocab_size)\n",
    "    print(\"Vocabs:\\t{:,}/{:,}\".format(len(vocabs), vocab_length))\n",
    "    # Word <==> id lookup tables\n",
    "    vocabs_w2id = w2id(vocabs, len(special_tokens))\n",
    "    vocabs_w2id.update(special_tokens)  # adding special tokens\n",
    "    vocabs_id2w = id2w(vocabs_w2id)\n",
    "\n",
    "    # Target sentences should be tailed by an <EOS> token\n",
    "    if is_target:\n",
    "        tokenized_sentences = append_EOS_token(tokenized_sentences)\n",
    "\n",
    "    # Converting word-lists into lists of word ids\n",
    "    sentences_of_ids = lists_of_ids(tokenized_sentences, vocabs_w2id)\n",
    "    return np.array(sentences_of_ids), vocabs_w2id, vocabs_id2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en => en\n",
      "fr => fr\n"
     ]
    }
   ],
   "source": [
    "files_dict = {}\n",
    "for f in files:\n",
    "    filename = f.get_info()['name']\n",
    "    key = \".\".join(filename.split(\".\")[-2:])\n",
    "    files_dict[key] = f\n",
    "    print(key, \"=>\" ,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\t137,861/137,861\n",
      "Vocabs:\t199/199\n",
      "Sentences:\t137,861/137,861\n",
      "Vocabs:\t339/339\n",
      "Time elapsed:\t4.514442443847656\n",
      "Data processed successfully\n"
     ]
    }
   ],
   "source": [
    "max_num_sentences = int(2e5)\n",
    "start_time = time.time()\n",
    "root = \"../../NLP_data/parallel/en-fr_small/\"\n",
    "src_text_ids, src_w2id, src_id2w = process_data(root+\"en\", tokenize,\n",
    "                                             max_num_sentences=max_num_sentences, max_vocab_size=10000)\n",
    "tar_text_ids, tar_w2id, tar_id2w = process_data(root+\"fr\", tokenize,\n",
    "                                                max_num_sentences=max_num_sentences, max_vocab_size=8000, is_target=True)\n",
    "print(\"Time elapsed:\\t{}\\nData processed successfully\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(\n",
    "    (\n",
    "        (src_text_ids, tar_text_ids),\n",
    "        (src_w2id, tar_w2id),(src_id2w, tar_id2w)\n",
    "    ), open('checkpoints/preprocess.p', 'wb')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess():\n",
    "    with open('checkpoints/preprocess.p', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(src_text_ids, tar_text_ids),\\\n",
    "(src_w2id, tar_w2id),(src_id2w, tar_id2w) = load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 136,861\n",
      "Validate: 1,000\n"
     ]
    }
   ],
   "source": [
    "permutation = np.random.permutation(len(src_text_ids))\n",
    "valid_size = min(len(src_text_ids)*5//100, 1000)\n",
    "# train_size = int(2e6)\n",
    "train_indices = permutation[valid_size:]\n",
    "valid_indices = permutation[:valid_size]\n",
    "\n",
    "src_train_text_ids = np.array(src_text_ids)[train_indices]\n",
    "src_valid_text_ids = np.array(src_text_ids)[valid_indices]\n",
    "\n",
    "tar_train_text_ids = np.array(tar_text_ids)[train_indices]\n",
    "tar_valid_text_ids = np.array(tar_text_ids)[valid_indices]\n",
    "\n",
    "assert len(src_train_text_ids) == len(tar_train_text_ids)\n",
    "assert len(src_valid_text_ids) == len(tar_valid_text_ids)\n",
    "print(\"Training: {:,}\".format(len(src_train_text_ids)))\n",
    "print(\"Validate: {:,}\".format(len(src_valid_text_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing EN-TR corpus stats here\n",
    "<p hidden>\n",
    "# print(\"Training Corpus\")\n",
    "# print(\"EN: \",end=\"\\t\")\n",
    "# src_train_text_ids, src_w2id, src_id2w = process_data(files_dict[\"en.train\"], tokenize)\n",
    "# print(\"TR: \",end=\"\\t\")\n",
    "# tar_train_text_ids, tarw2id, tarid2w = process_data(files_dict[\"tr.train\"], tokenize)\n",
    "\n",
    "# print()\n",
    "\n",
    "# print(\"Validation Corpus\")\n",
    "# print(\"EN: \",end=\"\\t\")\n",
    "# src_valid_text_ids, _, _ = process_data(files_dict[\"test.en\"], tokenize)\n",
    "# print(\"TR: \",end=\"\\t\")\n",
    "# tar_valid_text_ids, _, _ = process_data(files_dict[\"test.tr\"], tokenize)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `1` at the end of the target sentence ids list refers to the `<EOS>` token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: he likes bananas lemons and strawberries\n",
      "    [30, 73, 83, 79, 10, 77]\n",
      "FR: il aime les bananes les citrons et les fraises <EOS>\n",
      "    [6, 14, 7, 75, 7, 31, 9, 7, 73, 1]\n",
      "------------------------------\n",
      "EN: my least liked fruit is the pear but our least liked is the banana\n",
      "    [34, 15, 19, 16, 4, 8, 88, 9, 52, 15, 19, 4, 8, 86]\n",
      "FR: mon fruit est moins aime la poire mais notre moins aime est la banane <EOS>\n",
      "    [42, 19, 4, 17, 14, 10, 89, 8, 63, 17, 14, 4, 10, 92, 1]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    ix = np.random.randint(0, len(src_text_ids))\n",
    "    print(\"EN:\",''.join(sentence_ids_to_words(src_text_ids[ix], src_id2w)))\n",
    "    print(\"   \",src_text_ids[ix])\n",
    "    print(\"FR:\",''.join(sentence_ids_to_words(tar_text_ids[ix], tar_id2w)))\n",
    "    print(\"   \",tar_text_ids[ix])\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in the training corpus\n",
      "English: 203\n",
      "French: 343\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique words in the training corpus\\nEnglish: {:,}\\nFrench: {:,}\".format(len(src_w2id),len(tar_w2id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Seq2Seq model\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*_rSHLjFShknAu3jt3rbcNQ.png\">\n",
    "### 1) Input parameters to the encoder/decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_dec_model_inputs():\n",
    "    # 1st None for batch size, 2nd None is for sentence length\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targes')\n",
    "    \n",
    "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
    "    max_target_len = tf.reduce_max(target_sequence_length)\n",
    "    \n",
    "    return inputs, targets, target_sequence_length, max_target_len\n",
    "\n",
    "def hyperparam_inputs():\n",
    "    alpha = tf.placeholder_with_default(0.001, None, name='learning_rate')\n",
    "    keep_prob = tf.placeholder_with_default(0.5, None, name='dropout_keepprob')\n",
    "    \n",
    "    return alpha, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://nbviewer.jupyter.org/github/deep-diver/EN-FR-MLT-tensorflow/blob/master/go_insert.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omitted `process_decoder_input` function here\n",
    "<p hidden>\n",
    "    def add_go_token(target_data, go_id, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for encoding\n",
    "    :return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    \n",
    "    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n",
    "    \n",
    "    return after_concat\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_go_token = lambda target_input,go_id,batch_size: tf.concat([tf.fill([batch_size, 1], go_id), target_input], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Encoding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(source_ids, rnn_size, num_layers, keep_prob, vocab_size, embedding_size):\n",
    "    \"\"\"Is fed the source sequences and return the RNN output and RNN state\"\"\"\n",
    "    \n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "    \n",
    "    embed = tf.contrib.layers.embed_sequence(source_ids, vocab_size, embedding_size, initializer=he_init)\n",
    "    \n",
    "    layers = [tf.contrib.rnn.DropoutWrapper(\n",
    "                    tf.contrib.rnn.GRUCell(rnn_size), keep_prob)\n",
    "                          for _ in range(num_layers)]\n",
    "    \n",
    "    deep_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "    \n",
    "    outputs, states = tf.nn.dynamic_rnn(deep_cell, embed, dtype=tf.float32)\n",
    "    \n",
    "    return outputs, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Decoding layer\n",
    "#### a.  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_train(encoder_state,\n",
    "                         dec_cell,\n",
    "                         dec_embed_input,\n",
    "                         target_sequence_length,\n",
    "                         max_summary_length,\n",
    "                         output_layer,\n",
    "                         keep_prob):\n",
    "    \n",
    "    \"\"\" Creates the training process of the decoder\n",
    "        returns: BeamSearchDecoderOutput containing training logits and sample ids\"\"\"\n",
    "    \n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob=keep_prob)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, target_sequence_length)\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, helper, encoder_state, output_layer)\n",
    "    \n",
    "    # Unrolling the decoder (train) cell\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder, impute_finished=True, maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.  Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(encoder_state,\n",
    "                      dec_cell,\n",
    "                      dec_embeddings,\n",
    "                      go_id,\n",
    "                      eos_id,\n",
    "                      max_target_sequence_length,\n",
    "                      vocab_size,\n",
    "                      output_layer,\n",
    "                      batch_size,\n",
    "                      keep_prob):\n",
    "    \n",
    "    \"\"\" Creates the inference process of the decoder\n",
    "        returns: BasicDecoderOutput containing inference logits and sample ids\"\"\"\n",
    "    \n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob=keep_prob)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, tf.fill([batch_size], go_id), eos_id)\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, helper, encoder_state, output_layer)\n",
    "    \n",
    "    # Unrolling the decoder (inference) cell\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder, impute_finished=True, maximum_iterations=max_target_sequence_length)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Build the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_input,\n",
    "                   encoder_state,\n",
    "                   target_sequence_length,\n",
    "                   max_target_length,\n",
    "                   rnn_size,\n",
    "                   num_layers,\n",
    "                   target_w2id,\n",
    "                   batch_size,\n",
    "                   keep_prob,\n",
    "                   decoding_embedding_size\n",
    "                  ):\n",
    "    \n",
    "    target_vocab_size = len(target_w2id)\n",
    "    dec_embeddings = tf.Variable(\n",
    "                    tf.random_uniform([target_vocab_size, decoding_embedding_size])\n",
    "    )\n",
    "    embed = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    deep_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.GRUCell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    with tf.name_scope(\"decode\"):\n",
    "        output_layer = tf.layers.Dense(target_vocab_size)\n",
    "        train_output = decoder_train(\n",
    "            encoder_state,\n",
    "            deep_cell,\n",
    "            embed,\n",
    "            target_sequence_length,\n",
    "            max_target_length,\n",
    "            output_layer,\n",
    "            keep_prob\n",
    "        )\n",
    "        \n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        infer_output = decoder_inference(\n",
    "            encoder_state,\n",
    "            deep_cell,\n",
    "            dec_embeddings,\n",
    "            target_w2id[\"<GO>\"],\n",
    "            target_w2id[\"<EOS>\"],\n",
    "            max_target_length, target_vocab_size, output_layer, batch_size, keep_prob\n",
    "        )\n",
    "        \n",
    "    return train_output, infer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Build the Seq2Seq model\n",
    "- Connect the enconder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, batch_size, keep_prob,\n",
    "                  target_sequence_length, max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_w2id\n",
    "                 ):\n",
    "    \"\"\" Builds the Seq2Seq model\n",
    "        returns (Training BasicDecoderOutput, Inference BasicDecoderOutput)\"\"\"\n",
    "    enc_output, enc_state = encoding_layer(input_data,\n",
    "                                           rnn_size,\n",
    "                                           num_layers,\n",
    "                                           keep_prob,\n",
    "                                           source_vocab_size,\n",
    "                                           enc_embedding_size\n",
    "                                          )\n",
    "    dec_input = add_go_token(target_data, target_w2id[\"<GO>\"], batch_size)\n",
    "    \n",
    "    train_output, infer_output = decoding_layer(dec_input,\n",
    "                                                enc_state,\n",
    "                                                target_sequence_length,\n",
    "                                                max_target_sentence_length,\n",
    "                                                rnn_size, num_layers,\n",
    "                                                target_w2id,\n",
    "                                                batch_size,\n",
    "                                                keep_prob,\n",
    "                                                dec_embedding_size\n",
    "                                               )\n",
    "    return train_output, infer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Training\n",
    "-  Hyperparameters\n",
    "-  Define loss\n",
    "-  Optimize\n",
    "-  Apply gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 13\n",
    "batch_size = 128\n",
    "\n",
    "display_step = min(300, len(src_train_text_ids)//batch_size//3)  # display the stats 3 times per epoch\n",
    "\n",
    "rnn_size = 128\n",
    "num_layers = 3\n",
    "\n",
    "\n",
    "encoding_embedding_size = 200\n",
    "decoding_embedding_size = 200\n",
    "\n",
    "learning_rate = 0.001\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(\"checkpoints\",\"dev\")\n",
    "(src_text_ids, tar_text_ids),\\\n",
    "(src_w2id, tar_w2id),(src_id2w, tar_id2w) = load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "train_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    inputs, targets, target_sequence_length, max_target_len = enc_dec_model_inputs()\n",
    "    lr, keep_prob = hyperparam_inputs()\n",
    "    train_logits, inference_logits = seq2seq_model(inputs,\n",
    "                                                   targets,\n",
    "                                                   batch_size,\n",
    "                                                   keep_prob,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_len,\n",
    "                                                   len(src_w2id),\n",
    "                                                   len(tar_w2id),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   tar_w2id\n",
    "                                                   )\n",
    "    train_logits = tf.identity(train_logits.rnn_output, name=\"logits\")\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name=\"predictions\")\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n",
    "    # - Returns a mask tensor representing the first N positions of each cell.\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_len, dtype=tf.float32, name=\"masks\")\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss; Weighted Softmax Cross-entropy\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(train_logits, targets, masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "        #         optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "\n",
    "        # Grading clipping (to handle exploding gradients)\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://nbviewer.jupyter.org/github/deep-diver/EN-FR-MLT-tensorflow/blob/master/pad_insert.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the batches and pad the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_id):\n",
    "    max_sentence_len = max([len(s) for s in sentence_batch])\n",
    "    return [s + [pad_id]*(max_sentence_len - len(s)) for s in sentence_batch]\n",
    "\n",
    "def get_batches(source, target, batch_size, source_pad_id, target_pad_id, reverse_source=True):\n",
    "    for batch_i in range(0, len(source)//batch_size):\n",
    "        start = batch_i * batch_size\n",
    "        \n",
    "        # Get the batch\n",
    "        source_batch = source[start:start+batch_size]\n",
    "        target_batch = target[start:start+batch_size]\n",
    "        \n",
    "        # Pad\n",
    "        source_padded = pad_sentence_batch(source_batch, source_pad_id)\n",
    "        target_padded = pad_sentence_batch(target_batch, target_pad_id)\n",
    "        \n",
    "        # Reverse source sentence\n",
    "        if reverse_source:\n",
    "            source_padded = np.flip(source_padded, -1)\n",
    "        \n",
    "        source_lengths = [len(s) for s in source_padded]\n",
    "        target_lengths = [len(s) for s in target_padded]\n",
    "        \n",
    "        yield np.array(source_padded), np.array(target_padded), np.array(source_lengths), np.array(target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/1069 - Train Accuracy: 0.2574, Validation Accuracy: 0.2859, Loss: 5.8108\n",
      "Epoch   0 Batch  300/1069 - Train Accuracy: 0.4479, Validation Accuracy: 0.4705, Loss: 1.4956\n",
      "Epoch   0 Batch  600/1069 - Train Accuracy: 0.5352, Validation Accuracy: 0.5452, Loss: 0.9634\n",
      "Epoch   0 Batch  900/1069 - Train Accuracy: 0.6259, Validation Accuracy: 0.6118, Loss: 0.7159\n",
      "Epoch   1 Batch    0/1069 - Train Accuracy: 0.6228, Validation Accuracy: 0.6169, Loss: 0.6883\n",
      "Epoch   1 Batch  300/1069 - Train Accuracy: 0.6428, Validation Accuracy: 0.6466, Loss: 0.6050\n",
      "Epoch   1 Batch  600/1069 - Train Accuracy: 0.6328, Validation Accuracy: 0.6659, Loss: 0.5725\n",
      "Epoch   1 Batch  900/1069 - Train Accuracy: 0.6753, Validation Accuracy: 0.6786, Loss: 0.4527\n",
      "Epoch   2 Batch    0/1069 - Train Accuracy: 0.7270, Validation Accuracy: 0.7169, Loss: 0.4104\n",
      "Epoch   2 Batch  300/1069 - Train Accuracy: 0.7734, Validation Accuracy: 0.7518, Loss: 0.3451\n",
      "Epoch   2 Batch  600/1069 - Train Accuracy: 0.8060, Validation Accuracy: 0.7836, Loss: 0.3278\n",
      "Epoch   2 Batch  900/1069 - Train Accuracy: 0.8294, Validation Accuracy: 0.8354, Loss: 0.2620\n",
      "Epoch   3 Batch    0/1069 - Train Accuracy: 0.8733, Validation Accuracy: 0.8524, Loss: 0.2265\n",
      "Epoch   3 Batch  300/1069 - Train Accuracy: 0.8689, Validation Accuracy: 0.8669, Loss: 0.2062\n",
      "Epoch   3 Batch  600/1069 - Train Accuracy: 0.8759, Validation Accuracy: 0.8755, Loss: 0.2012\n",
      "Epoch   3 Batch  900/1069 - Train Accuracy: 0.9132, Validation Accuracy: 0.8894, Loss: 0.1577\n",
      "Epoch   4 Batch    0/1069 - Train Accuracy: 0.9132, Validation Accuracy: 0.8993, Loss: 0.1474\n",
      "Epoch   4 Batch  300/1069 - Train Accuracy: 0.8950, Validation Accuracy: 0.9039, Loss: 0.1263\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_accs = dict()\n",
    "valid_accs = dict()\n",
    "loss_valus = dict()\n",
    "\n",
    "# Actual training\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batches_per_epoch = len(src_train_text_ids) // batch_size\n",
    "    for epoch in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, source_lengths, target_lengths) in enumerate(\n",
    "                get_batches(\n",
    "                    src_train_text_ids,\n",
    "                    tar_train_text_ids,\n",
    "                    batch_size,\n",
    "                    src_w2id[\"<PAD>\"],\n",
    "                    tar_w2id[\"<PAD>\"],\n",
    "                    reverse_source=True\n",
    "                )\n",
    "        ):\n",
    "            _, loss = sess.run([train_op, cost],\n",
    "                               {inputs: source_batch,\n",
    "                                targets: target_batch,\n",
    "                                lr: learning_rate,\n",
    "                                keep_prob: keep_probability,\n",
    "                                target_sequence_length: target_lengths\n",
    "                                })\n",
    "\n",
    "            if batch_i % display_step == 0:\n",
    "                acc_train = 0\n",
    "                acc_valid = 0\n",
    "                count = 0\n",
    "                for (valid_sources_batch, valid_targets_batch, valid_sources_lengths,\n",
    "                     valid_targets_lengths) in get_batches(\n",
    "                        src_valid_text_ids,\n",
    "                        tar_valid_text_ids,\n",
    "                        batch_size,\n",
    "                        src_w2id['<PAD>'],\n",
    "                        tar_w2id['<PAD>']\n",
    "                ):\n",
    "                    train_batch_logits = sess.run(inference_logits,\n",
    "                                                  {inputs: source_batch,\n",
    "                                                   targets: target_batch,\n",
    "                                                   target_sequence_length:target_lengths,\n",
    "                                                   keep_prob: 1.0\n",
    "                                                   })\n",
    "                    valid_batch_logits = sess.run(inference_logits,\n",
    "                                                  {inputs: valid_sources_batch,\n",
    "                                                   targets: valid_targets_batch,\n",
    "                                                   target_sequence_length:valid_targets_lengths,\n",
    "                                                   keep_prob: 1.0\n",
    "                                                   })\n",
    "                    acc_train += get_accuracy(target_batch, train_batch_logits)\n",
    "                    acc_valid += get_accuracy(valid_targets_batch, valid_batch_logits)\n",
    "                    count += 1\n",
    "\n",
    "                acc_train /= count\n",
    "                acc_valid /= count\n",
    "\n",
    "                key = epoch * batches_per_epoch + batch_i\n",
    "                train_accs[key] = acc_train\n",
    "                valid_accs[key] = acc_valid\n",
    "                loss_valus[key] = loss\n",
    "\n",
    "                print(\n",
    "                    'Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                    .format(epoch, batch_i, batches_per_epoch, acc_train, acc_valid, loss))\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved\\nElapsed time: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_params(params, train_accs, valid_accs, loss_vals, batch_size):\n",
    "    with open('params.p', 'wb') as out_file:\n",
    "        pickle.dump({\"load_path\": params,\n",
    "                     \"train_accs\": train_accs,\n",
    "                     \"valid_accs\": valid_accs,\n",
    "                     \"loss_vals\": loss_vals,\n",
    "                     \"batch_size\":batch_size\n",
    "                     }, out_file)\n",
    "\n",
    "\n",
    "def load_params(params_path='params.p'):\n",
    "    with open(params_path, mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the path of checkpoint parameters\n",
    "save_params(save_path, train_accs, valid_accs, loss_vals, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = load_params()\n",
    "\n",
    "get_y_vals = lambda d: [d[i] for i in sorted(d)]\n",
    "xticks = sorted(params[\"train_accs\"])\n",
    "\n",
    "train_accs = get_y_vals(xticks)\n",
    "valid_accs = get_y_vals(params[\"valid_accs\"])\n",
    "loss_valus = get_y_vals(params[\"loss_valus\"])\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12,7))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Accuracy', color=color)\n",
    "\n",
    "ax1.plot(xticks, train_accs, color=color)\n",
    "ax1.plot(xticks, valid_accs, color='tab:green')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Cross-entropy loss', color=color)\n",
    "ax2.plot(xticks, loss_vals, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax1.legend(['Train. accuracy', 'Valid. accuracy'], loc='upper center')\n",
    "ax2.legend(['Cross-entropy loss'])\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation checking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(src_text_ids, tar_text_ids),\\\n",
    "(src_w2id, tar_w2id),(src_id2w, tar_id2w) = load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, params_path='params.p', language=\"english\"):\n",
    "    params = load_params(params_path)\n",
    "\n",
    "    load_path = params[\"load_path\"]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "\n",
    "    (src_text_ids, tar_text_ids), \\\n",
    "    (source_w2id, tar_w2id), (source_id2w, tar_id2w) = load_preprocess(load_path)\n",
    "\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/{}.pickle'.format(language))\n",
    "\n",
    "    raw_sentences = tokenize_sentences(sentence_to_wordlist, tokenizer.tokenize(text.lower()))\n",
    "\n",
    "    translate_sentence_ids = lists_of_ids(raw_sentences, source_w2id)\n",
    "\n",
    "    num_sentences = len(translate_sentence_ids)\n",
    "\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        loader = tf.train.import_meta_graph(load_path + \".meta\")\n",
    "        loader.restore(sess, load_path)\n",
    "\n",
    "        inputs = loaded_graph.get_tensor_by_name(\"inputs:0\")\n",
    "        logits = loaded_graph.get_tensor_by_name(\"predictions:0\")\n",
    "        target_sequence_length = loaded_graph.get_tensor_by_name(\"target_sequence_length:0\")\n",
    "        keep_prob = loaded_graph.get_tensor_by_name(\"dropout_keepprob:0\")\n",
    "\n",
    "\n",
    "        input_sentences = translate_sentence_ids+[\"\"]*(batch_size-num_sentences)\n",
    "\n",
    "        # target_sequence_length is set to 2x the len of the source sentence to assure the translation fits in\n",
    "        translate_logits = sess.run(logits, {inputs:input_sentences,\n",
    "                                             target_sequence_length: [2 * len(s) for s in input_sentences],\n",
    "                                             keep_prob: 1.0\n",
    "                                             })\n",
    "    return translate_logits[:num_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"I love yellow trucks\"\n",
    "translated_sentence = \" \".join([tar_id2w[w] for w in translate(input_sentence, src_w2id, special_tokens)])\n",
    "print()\n",
    "print(\"EN:\", input_sentence)\n",
    "print(\"FR: \", translated_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
